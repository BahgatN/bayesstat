---
title: "t-Test Bayesian Estimation"
author: "Bahgat Nassour"
output: html_document
---

## Basics of Bayesian inference 

Bayes's Rule formula 

$$ P(A|B) = \frac{P(A)P(B|A)}{P(B)} $$
this classical formula is the fundamental in the statistical Bayesian inference theory

$$ P(\theta|x) = \frac{P(\theta)P(x|\theta)}{P(x)} $$
let $f ()$ denote a probability or a probability density function then 
$$ f(\theta|x) = \frac{f(\theta)f(x|\theta)}{f(x)} =  \frac{f(\theta)L(\theta;x)}{\int f(\theta)f(x|\theta)d\theta} $$
\newline
Under this formula we can see the first fundamental difference between the Bayesian approach and the frequentist approach, in Bayesian inference we treat the data $x$ as fixed and the parameter of interest $\theta$ as random variable, that means $\theta$ has a distribution

\newline
Remember that we have learned in beginner class of statistics and probability that there are different definitions of probability that have been suggested in the course of the development of probability theory two of these definitions are called relative frequency definition of probability and subjective definition of probability.

\newline
So treating data as fixed and the parameter as random in Bayesian inference will radically change the intereprtation of very basics concepts in statisics like for example $p$ value, hypothesis testing or condidence interval ..etc.  

\newline
Back to the formula

\newline
$f(\theta)$ is the prior distribution 

\newline
$f(\theta|x)$ is the posterior distribution  

\newline
$L(\theta;x)$ is the Likelihood 

\newline
$f(x)$ is the marginal distribution of the data $x$ or the the normalizing constant and it is less important in Bayesian inference since it is a function of the $x$ which are fixed, dropping out this  normalizing constant leads the posterior density $f(\theta|x)$ to lose some properties like integration to 1 (improper density ) over the domain of $\theta$ but this is not a big deal since we are usually not interested in integrating the function but ratherin maximising it, so multiplying this function with the constant does not change the $\theta$ that corresponds to the maximum point (MAP), in this case we can say that the posterior is proportional (not equal) to prior multiplyed with likelihood i.e $f(\theta|x) \propto f(\theta) L(\theta;x)$

## Conjugate priors, the Posisson model as an example
Suppose we have a random variable $X$ has a Poisson distribution with mean $\theta$ i.e. 
$$ Pr(X=x|\theta) = \frac{\theta^{x} e^ {-\theta}}{x!} : x \in\{0, 1, 2, ...\} $$
As we know for such random variable $E[X|\theta] = \theta$ and $Var[X|\theta] = \theta$

\newline
Now suppose that there is a sample $x_{1},x_{2} . . . , x_{n}$ of $n$ independent and identically distributed (iid) observations or realization of the random variable $X \sim pois(\theta)$, then joint density function pdf or the likelihood function is 
$$f(x_1,x_2,...,x_n|\theta) = \prod_{i=1}^{i=n}f(x_i|\theta) =  \prod_{i=1}^{i=n} \frac{1}{x_{i}!}\theta^{x_{i}}e^{-\theta}$$
\newline 
Now we will work with a class of conjugate prior distributions that will
make posterior calculations simple.

\newline 

In general a class of prior densities is conjugate for a sampling model $p(x_1,x_2,...,x_n|\theta)$ if the posterior distribution is also in that class. For the Poisson sampling model, our posterior distribution for $\theta$ has the following form:

$$p(\theta|x) \propto p(\theta) p(x_1,x_2,...,x_n|\theta)$$
$$\propto p(\theta)\theta^{\sum y_{i}}e^{-n\theta}$$
This means that whatever our conjugate class of densities is, it will have
to include terms like $\theta^{c_{1}}e^{-c_{2}\theta}$ for numbers $c_{1}$ and $c_{2}$.The simplest class of such densities includes only these terms, and their corresponding probability
distributions are known as the family of gamma distributions.

\newline
Again we want to infer about the Posisson pdf parameter namely $\theta$ given the data at hand, this uncertain positive quantity $\theta$ has a gamma distribution with two parameters $\alpha$ and $\beta$,i.e.
$$p(\theta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta} : \alpha, \beta > 0$$
this is anyway the prior distribution which reflects our prior information or beleif about $\theta$,this distribution could take diffrent shapes based on the two parameters $\alpha$ and $\beta$ as we see below ,

\newline
```{r pressure, echo=FALSE}
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(2,3))

a<-1 ; b<-1
x<-seq(.001,10,length=100)
plot(x, dgamma(x,a,b),type="l",
     xlab=expression(theta), ylab=expression(italic(paste("p(",theta,")",sep=""))))
mtext(expression(italic(paste(alpha," = 1 ",beta," = 1",sep=""))),side=3,line=.12,cex=.8)

a<-2 ; b<-2
x<-seq(.001,10,length=100)
plot(x, dgamma(x,a,b),type="l",
     xlab=expression(theta), ylab=expression(italic(paste("p(",theta,")",sep=""))))
mtext(expression(italic(paste(alpha," = 2 ",beta," = 2",sep=""))),side=3,line=.12,cex=.8)

a<-4 ; b<-4
x<-seq(.001,10,length=100)
plot(x, dgamma(x,a,b),type="l",
     xlab=expression(theta), ylab=expression(italic(paste("p(",theta,")",sep=""))))
mtext(expression(italic(paste(alpha," = 4 ",beta," = 4",sep=""))),side=3,line=.12,cex=.8)

a<-2 ; b<-1
x<-seq(.001,10,length=100)
plot(x, dgamma(x,a,b),type="l",
     xlab=expression(theta), ylab=expression(italic(paste("p(",theta,")",sep=""))))
mtext(expression(italic(paste(alpha," = 2 ",beta," = 1",sep=""))),side=3,line=.12,cex=.8)

a<-8 ; b<-4
x<-seq(.001,10,length=100)
plot(x, dgamma(x,a,b),type="l",
     xlab=expression(theta), ylab=expression(italic(paste("p(",theta,")",sep=""))))
mtext(expression(italic(paste(alpha," = 8 ",beta," = 4",sep=""))),side=3,line=.12,cex=.8)

a<-32 ; b<-16
x<-seq(.001,10,length=100)
plot(x, dgamma(x,a,b),type="l",
     xlab=expression(theta), ylab=expression(italic(paste("p(",theta,")",sep=""))))
mtext(expression(italic(paste(alpha," = 32 ",beta," = 16",sep=""))),side=3,line=.12,cex=.8)
```

\newline 
Now we are done and we just have to calculate analytically the posterior distribution of $\theta$.

$$  p(\theta|x_1,x_2,...,x_n) = \frac{p(\theta) p(x_1,x_2,...,x_n|\theta)}{p(x_1,x_2,...,x_n)}$$
$$ p(\theta|x_1,x_2,...,x_n)  = (\theta^{\alpha-1}e^{-\beta\theta})(\theta^{\sum y_{i}}e^{-n\theta})(c(x_1,x_2,...,x_n,\alpha, \beta))$$
$$ p(\theta|x_1,x_2,...,x_n)  = (\theta^{\alpha + \sum y_{i} - 1}e^{(-b+n)\theta}) (c(x_1,x_2,...,x_n,\alpha, \beta))$$
$$ p(\theta|x_1,x_2,...,x_n) \propto (\theta^{\alpha + \sum y_{i} - 1}e^{(-b+n)\theta}) $$

This is evidently a gamma distribution i.e $(\theta|x_1,x_2,...,x_n) \sim gamma(\alpha + \sum y_{i}, n + \beta )$
The posterior expectation of $\theta$ is a convex combination of the prior expectation and the sample average:
$$ E[\theta|x_1,x_2,...,x_n] = \frac{a + \sum y_{i}}{b + n}$$
Letâ€™s try to obtain a deeper understanding by takeing a real world example.
## Posterior numerical integration 

## Quick review of BEST R Package


